import json
from wanikani import WaniKaniHandler

from config import WANIKANI_API_TOKEN, JPDB_API_TOKEN

FREQUENCY_LIST_FILE = "Frequency_List.json"
FREQ_SOURCE_FILE = "BCCWJ_frequencylist_suw_ver1_0.tsv"
BLACKLISTED_WORD_TYPES = ["助詞", "助動詞", "接尾辞", "数詞"]
KANA_LIST = [
    'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ゔ', 'ぇ', 'え', 'ぉ', 'お', 'ゕ', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'ゖ', 'け', 'げ',
    'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で',
    'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ',
    'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'ゐ', 'ゑ',
    'を', 'ん', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ヴ', 'ェ', 'エ', 'ォ', 'オ', 'ヵ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ヶ',
    'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ',
    'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ベ', 'ペ', 'ホ',
    'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ヷ',
    'ヰ', 'ヸ', 'ヱ', 'ヹ', 'ヲ', 'ヺ', 'ン', '・', 'ー'
]


def main():
    # === Regenerate Frequency List File
    # generate_frequency_list()

    wanikanihandler = WaniKaniHandler(WANIKANI_API_TOKEN)
    wanikanihandler.download_all_data()
    wanikanihandler.write_cache()

    known_kanji_list = get_known_kanji_list(data_cache_json)
    known_vocabulary_list = get_known_vocabulary_list(data_cache_json)

    # candidate_words_list = get_candidate_words_list(known_kanji_list, 5000)
    # print("Generated list of %d candidate words:\n" % len(candidate_words_list) + str(candidate_words_list))
    # filtered_words_list = filter_out_known_vocabulary(candidate_words_list, known_vocabulary_list)
    # print("Generated list of %d filtered words:\n" % len(filtered_words_list) + str(filtered_words_list))

def generate_frequency_list() -> None:
    """
    Populate Frequency List file from the BCCWJ database file
    Filters out words that are categorized as any of the Blacklisted Types
    """
    with open(FREQUENCY_LIST_FILE, "w") as frequency_list_file:
        list_of_words = []
        with open(FREQ_SOURCE_FILE, "r", encoding='utf-8') as freq_source_file:
            for line in freq_source_file:
                data = line.split('\t')
                word = data[2]
                word_type = data[3]
                word_is_blacklisted = False
                for blacklisted_type in BLACKLISTED_WORD_TYPES:
                    if blacklisted_type in word_type:
                        word_is_blacklisted = True
                        break
                if not word_is_blacklisted:
                    list_of_words.append(word)
        json.dump(
            list_of_words[1:],
            frequency_list_file,
            indent=0
        )

def get_known_kanji_list(data_cache_json: dict[str, dict]) -> list[str]:
    """
    :param data_cache_json: Object containing entries "user_kanji_assignments" and "all_kanji_subjects" as generated by the download functions
    :return: All the user's known kanji in Unicode
    """
    user_kanji_assignments = data_cache_json["user_kanji_assignments"]
    all_kanji_subjects = data_cache_json["all_kanji_subjects"]
    known_kanji_list = []
    for lesson_id in user_kanji_assignments:
        known_kanji_list.append(all_kanji_subjects[lesson_id])
    return known_kanji_list

def get_known_vocabulary_list(data_cache_json: dict[str, dict]) -> list[str]:
    """
    :param data_cache_json: Object containing entries "user_vocabulary_assignments" and "all_vocabulary_subjects" as generated by the download functions
    :return: All the user's known vocabulary in Unicode
    """
    user_vocabulary_assignments = data_cache_json["user_vocabulary_assignments"]
    all_vocabulary_subjects = data_cache_json["all_vocabulary_subjects"]
    known_vocabulary_list = []
    for lesson_id in user_vocabulary_assignments:
        known_vocabulary_list.append(all_vocabulary_subjects[lesson_id])
    return known_vocabulary_list

def get_candidate_words_list(known_kanji_list: list[str], up_to_frequency: int) -> list[str]:
    """
    :param known_kanji_list: All the known kanji to a user
    :param up_to_frequency: The number of words to be searched (e.g. 500 = the 500 most common words are searched)
    :return: Words that only contain known kanji (and kana)
    """
    known_character_list = KANA_LIST + known_kanji_list  # Append hiragana/katakana to known kanji for filtering
    with open(FREQUENCY_LIST_FILE, "r", encoding='utf-8') as frequency_list_file:
        words_list = json.load(frequency_list_file)
        if len(words_list) < up_to_frequency:  # Cap up_to_frequency to the length of word_list
            print("Frequency list doesn't contain %d items. Only searching %d.", (up_to_frequency, len(words_list)))
            up_to_frequency = len(words_list)

        candidate_words_list = []
        for i in range(up_to_frequency):
            current_word = words_list[i]

            is_candidate_word = True  # Check if word is composed only of known kanji TODO: this is janky
            for current_kanji in current_word:
                if current_kanji not in known_character_list:
                    is_candidate_word = False
                    break

            if is_candidate_word:
                candidate_words_list.append(current_word)
        return candidate_words_list

def filter_out_known_vocabulary(candidate_words_list: list[str], known_vocabulary_list: list[str]) -> list[str]:
    """
    :param candidate_words_list: Words to be checked
    :param known_vocabulary_list: Words to be filtered out of candidate_words_list
    :return: New list containing words that passed the filter
    """
    filtered_candidate_words_list = []
    for word in candidate_words_list:
        if word not in known_vocabulary_list:
            filtered_candidate_words_list.append(word)
    return filtered_candidate_words_list


if __name__ == "__main__":
    main()
